{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSwMJ1aZZB3G"
      },
      "source": [
        "# Email Fraud Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub google-generativeai pandas nltk"
      ],
      "metadata": {
        "id": "4TF0mgl4LsjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1c2bd2-0ca7-44e8-870d-62e0a5736f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.151.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.23.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required dependencies\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import kagglehub\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import caching\n",
        "from typing import List, Dict, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Download required NLTK data with proper error handling\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    logging.info(\"Successfully downloaded NLTK punkt resource\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error downloading NLTK resources: {str(e)}\")\n",
        "    # Don't raise here - punkt might already be downloaded\n"
      ],
      "metadata": {
        "id": "yHmQMJ0Wmyzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLihmhDMZB3L"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Simple configuration class for email analysis parameters\"\"\"\n",
        "    # TESTING VALUE: Reduced to 1/10th for development\n",
        "    # Original value was 1000, restore for production\n",
        "    chunk_size: int = 100            # How many emails to process at once\n",
        "    max_retries: int = 3             # Number of retries if API call fails\n",
        "    min_message_length: int = 10     # Minimum length of message to process\n",
        "    api_timeout: int = 30            # Timeout for API calls in seconds\n",
        "    cache_ttl: int = 3600           # Cache lifetime in seconds (1 hour)\n",
        "    model_name: str = 'gemini-1.5-flash-001'\n",
        "\n",
        "# Create a single configuration instance with default values\n",
        "config = Config()\n",
        "\n",
        "# You can easily modify any settings here if needed\n",
        "# For example:\n",
        "# config.chunk_size = 500  # Process smaller chunks\n",
        "# config.min_message_length = 20  # Require longer messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i68wkF2YZB3L"
      },
      "outputs": [],
      "source": [
        "# Set up Gemini API\n",
        "os.environ['API_KEY'] = 'AIzaSyBPv2RhmsMT0HPcMyfibd2ELTuCAiCH1j0'  # Replace with your API key\n",
        "genai.configure(api_key=os.environ['API_KEY'])\n",
        "\n",
        "system_instruction = \"\"\"\n",
        "You are an expert fraud analyst.\n",
        "Your task is to analyze the provided email chunks\n",
        "and identify potential evidence of fraudulent activities.\n",
        "Focus on:\n",
        "1. Financial irregularities\n",
        "2. Suspicious transactions\n",
        "3. Attempts to conceal information\n",
        "4. Unusual communication patterns\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_message(message: str) -> str:\n",
        "    \"\"\"Clean and standardize message text\"\"\"\n",
        "    if not isinstance(message, str):\n",
        "        return ''\n",
        "\n",
        "    message = message.strip()\n",
        "    if len(message) < config.min_message_length:\n",
        "        return ''\n",
        "\n",
        "    return message\n",
        "\n",
        "def chunk_message(message: str, max_chunk_size: int = 1000) -> List[str]:\n",
        "    \"\"\"Split message into smaller chunks while preserving context\"\"\"\n",
        "    if not message:\n",
        "        return []\n",
        "\n",
        "    sentences = message.split('. ')  # Simple sentence splitting\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_size = len(sentence.split())\n",
        "        if current_size + sentence_size > max_chunk_size and current_chunk:\n",
        "            chunks.append('. '.join(current_chunk) + '.')\n",
        "            current_chunk = []\n",
        "            current_size = 0\n",
        "        current_chunk.append(sentence)\n",
        "        current_size += sentence_size\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append('. '.join(current_chunk) + '.')\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "XDbFXqB9nRNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV Processing\n",
        "def create_caches_from_csv(csv_path: str, chunk_size: int = None) -> List[caching.CachedContent]:\n",
        "    \"\"\"Process CSV data in chunks with proper validation and error handling\"\"\"\n",
        "    chunk_size = chunk_size or config.chunk_size\n",
        "\n",
        "    # Set a conservative token limit (30% of the max)\n",
        "    MAX_TOKENS_PER_CACHE = 300000\n",
        "    # Set total token limit across all caches (90% of API limit)\n",
        "    TOTAL_TOKEN_LIMIT = 900000\n",
        "    model = genai.GenerativeModel(config.model_name)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        required_columns = ['file', 'message']\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            missing = [col for col in required_columns if col not in df.columns]\n",
        "            raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "        caches = []\n",
        "        num_chunks = len(df) // chunk_size + (len(df) % chunk_size > 0)\n",
        "\n",
        "        # ============= TESTING LIMIT =============\n",
        "        # REMOVE THIS LINE FOR PRODUCTION USE\n",
        "        num_chunks = min(3, num_chunks)  # Limit to first 3 chunks for testing\n",
        "        # ======================================\n",
        "\n",
        "        # Count system instruction tokens once\n",
        "        system_tokens = model.count_tokens(system_instruction).total_tokens\n",
        "        print(f\"System instruction uses {system_tokens} tokens\")\n",
        "\n",
        "        total_tokens_used = 0\n",
        "\n",
        "        for chunk_num in range(num_chunks):\n",
        "            # Check if we're approaching total token limit\n",
        "            if total_tokens_used >= TOTAL_TOKEN_LIMIT:\n",
        "                print(f\"\\nReached total token limit ({total_tokens_used} tokens) - stopping processing\")\n",
        "                break\n",
        "\n",
        "            start_idx = chunk_num * chunk_size\n",
        "            end_idx = min((chunk_num + 1) * chunk_size, len(df))\n",
        "\n",
        "            chunk = df.iloc[start_idx:end_idx].copy()\n",
        "            chunk.loc[:, 'message'] = chunk['message'].fillna('').astype(str)\n",
        "            chunk.loc[:, 'message'] = chunk['message'].apply(preprocess_message)\n",
        "            chunk = chunk[chunk['message'].str.len() > 0]\n",
        "\n",
        "            if not chunk.empty:\n",
        "                processed_messages = []\n",
        "                current_token_count = system_tokens  # Start with system tokens\n",
        "                print(f\"\\nProcessing chunk {chunk_num + 1}/{num_chunks}\")\n",
        "\n",
        "                for msg in chunk['message']:\n",
        "                    msg_chunks = chunk_message(msg)\n",
        "                    for msg_chunk in msg_chunks:\n",
        "                        # Get exact token count for this chunk\n",
        "                        chunk_tokens = model.count_tokens(msg_chunk).total_tokens\n",
        "\n",
        "                        # Add safety margin for potential overhead (20%)\n",
        "                        estimated_total = current_token_count + chunk_tokens + (chunk_tokens * 0.2)\n",
        "\n",
        "                        if estimated_total > MAX_TOKENS_PER_CACHE:\n",
        "                            print(f\"Reached token limit at {current_token_count} tokens - creating cache...\")\n",
        "                            break\n",
        "\n",
        "                        processed_messages.append(msg_chunk)\n",
        "                        current_token_count += chunk_tokens\n",
        "\n",
        "                    # Break out of outer loop too if we hit the limit\n",
        "                    if estimated_total > MAX_TOKENS_PER_CACHE:\n",
        "                        break\n",
        "\n",
        "                if processed_messages:\n",
        "                    try:\n",
        "                        # Check if adding this cache would exceed total limit\n",
        "                        if total_tokens_used + current_token_count > TOTAL_TOKEN_LIMIT:\n",
        "                            print(f\"\\nWould exceed total token limit - stopping at {total_tokens_used} tokens\")\n",
        "                            break\n",
        "\n",
        "                        cache = caching.CachedContent.create(\n",
        "                            model=config.model_name,\n",
        "                            display_name=f'fraud_analysis_chunk_{chunk_num}',\n",
        "                            system_instruction=system_instruction,\n",
        "                            contents=[{'text': msg} for msg in processed_messages],\n",
        "                            ttl=datetime.timedelta(seconds=config.cache_ttl)\n",
        "                        )\n",
        "                        caches.append(cache)\n",
        "                        total_tokens_used += current_token_count\n",
        "                        print(f\"Successfully created cache {chunk_num + 1} with {current_token_count} tokens (Total: {total_tokens_used})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Failed to create cache for chunk {chunk_num + 1}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "        if not caches:\n",
        "            raise ValueError(\"No valid caches could be created from the input data\")\n",
        "\n",
        "        print(f\"\\nSuccessfully processed {len(caches)} chunks (Total tokens: {total_tokens_used})\")\n",
        "        return caches\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing CSV: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "-Ozj2LdNncA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Email Processing\n",
        "def process_email_batch(emails: List[Dict], retries: Optional[int] = None) -> List[Dict]:\n",
        "    \"\"\"Process a batch of emails with retry logic\"\"\"\n",
        "    retries = retries or config.max_retries\n",
        "    results = []\n",
        "\n",
        "    for email in emails:\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                # Create a model instance for this analysis\n",
        "                model = genai.GenerativeModel(config.model_name)\n",
        "\n",
        "                # Analyze the email\n",
        "                response = model.generate_content([\n",
        "                    system_instruction,\n",
        "                    f\"From: {email.get('sender', 'Unknown')}\\n\"\n",
        "                    f\"To: {email.get('recipient', 'Unknown')}\\n\"\n",
        "                    f\"Subject: {email.get('subject', 'No Subject')}\\n\"\n",
        "                    f\"Body: {email.get('body', 'No Content')}\"\n",
        "                ])\n",
        "\n",
        "                results.append({\n",
        "                    'file': f\"{email.get('sender', 'Unknown')} - {email.get('subject', 'No Subject')}\",\n",
        "                    'analysis': response.text,\n",
        "                    'status': 'success'\n",
        "                })\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt == retries - 1:\n",
        "                    logging.error(f\"Failed to process email: {str(e)}\")\n",
        "                    results.append({\n",
        "                        'file': f\"{email.get('sender', 'Unknown')} - {email.get('subject', 'No Subject')}\",\n",
        "                        'error': str(e),\n",
        "                        'status': 'failed'\n",
        "                    })\n",
        "                time.sleep(attempt * 2)  # Exponential backoff\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "mdv-4_TKnsKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Console Interface\n",
        "def analyze_file(file=None, case_details=None, system_prompt=None, keywords=None):\n",
        "    \"\"\"Analyze emails for potential fraud using Gemini AI.\"\"\"\n",
        "    try:\n",
        "        # Use the default dataset\n",
        "        path = kagglehub.dataset_download(\"wcukierski/enron-email-dataset\")\n",
        "        file_path = os.path.join(path, 'emails.csv')\n",
        "        print(f\"Using default email dataset from: {file_path}\")\n",
        "\n",
        "        # Use custom system prompt if provided\n",
        "        current_prompt = system_prompt if system_prompt else system_instruction\n",
        "\n",
        "        # Create caches from the CSV file\n",
        "        caches = create_caches_from_csv(file_path)\n",
        "\n",
        "        # Process emails using the cached content\n",
        "        sample_results = []\n",
        "        for cache in caches[:1]:  # Process first chunk for sample results\n",
        "            try:\n",
        "                model = genai.GenerativeModel.from_cached_content(cache)\n",
        "\n",
        "                df = pd.read_csv(file_path)\n",
        "                chunk_size = config.chunk_size\n",
        "                chunk = df.iloc[0:chunk_size].copy()\n",
        "\n",
        "                chunk.loc[:, 'message'] = chunk['message'].fillna('').astype(str)\n",
        "                chunk.loc[:, 'message'] = chunk['message'].apply(preprocess_message)\n",
        "                chunk = chunk[chunk['message'].str.len() > 0]\n",
        "\n",
        "                email_texts = [\n",
        "                    f\"EMAIL {i+1}:\\n{msg}\\n---\"\n",
        "                    for i, msg in enumerate(chunk['message']) if msg.strip()\n",
        "                ]\n",
        "\n",
        "                analysis_prompt = [\n",
        "                    current_prompt,\n",
        "                    \"Below are the email chunks to analyze:\",\n",
        "                    \"\\n\".join(email_texts)\n",
        "                ]\n",
        "\n",
        "                response = model.generate_content(analysis_prompt)\n",
        "\n",
        "                sample_results.append({\n",
        "                    'file': 'Sample Analysis',\n",
        "                    'analysis': response.text,\n",
        "                    'status': 'success'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process cached content: {str(e)}\")\n",
        "                sample_results.append({\n",
        "                    'file': 'Error',\n",
        "                    'error': str(e),\n",
        "                    'status': 'failed'\n",
        "                })\n",
        "\n",
        "        output_data = []\n",
        "        for result in sample_results:\n",
        "            row = {\n",
        "                'File': result['file'],\n",
        "                'Status': result['status'],\n",
        "                'Analysis': result.get('analysis', result.get('error', ''))\n",
        "            }\n",
        "            output_data.append(row)\n",
        "\n",
        "        results_df = pd.DataFrame(output_data)\n",
        "\n",
        "        if keywords:\n",
        "            for keyword in keywords.split(','):\n",
        "                keyword = keyword.strip()\n",
        "                if keyword:\n",
        "                    results_df['Analysis'] = results_df['Analysis'].str.replace(\n",
        "                        f'({keyword})',\n",
        "                        r'**\\1**',  # Using markdown-style bold\n",
        "                        case=False,\n",
        "                        regex=True\n",
        "                    )\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return pd.DataFrame([{\n",
        "            'File': 'Error',\n",
        "            'Status': 'failed',\n",
        "            'Analysis': error_msg\n",
        "        }])"
      ],
      "metadata": {
        "id": "uyF4Hiq1Fkgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Get custom prompt if desired\n",
        "    print(\"\\nCurrent system prompt:\")\n",
        "    print(system_instruction)\n",
        "    custom_prompt = input(\"\\nEnter custom system prompt (or press Enter to use default): \").strip()\n",
        "\n",
        "    # Get keywords for highlighting\n",
        "    keywords = input(\"\\nEnter keywords to highlight (comma-separated, or press Enter to skip): \").strip()\n",
        "\n",
        "    # Run analysis\n",
        "    results_df = analyze_file(\n",
        "        file=None,\n",
        "        case_details=\"Email Analysis\",\n",
        "        system_prompt=custom_prompt if custom_prompt else None,\n",
        "        keywords=keywords\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n=== Analysis Results ===\\n\")\n",
        "    for _, row in results_df.iterrows():\n",
        "        print(f\"File: {row['File']}\")\n",
        "        print(f\"Status: {row['Status']}\")\n",
        "        print(\"Analysis:\")\n",
        "        print(row['Analysis'])\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I29aD6BVFmPT",
        "outputId": "fa96002c-57bc-402f-b87c-2b33aa046130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current system prompt:\n",
            "\n",
            "You are an expert fraud analyst.\n",
            "Your task is to analyze the provided email chunks\n",
            "and identify potential evidence of fraudulent activities.\n",
            "Focus on:\n",
            "1. Financial irregularities\n",
            "2. Suspicious transactions\n",
            "3. Attempts to conceal information\n",
            "4. Unusual communication patterns\n",
            "\n",
            "\n",
            "Enter custom system prompt (or press Enter to use default): look for frogs, bogs, dogs, nogs, bogs, logs or any other ogs.\n",
            "\n",
            "Enter keywords to highlight (comma-separated, or press Enter to skip): \n",
            "Using default email dataset from: /root/.cache/kagglehub/datasets/wcukierski/enron-email-dataset/versions/2/emails.csv\n",
            "System instruction uses 56 tokens\n",
            "\n",
            "Processing chunk 1/3\n",
            "Successfully created cache 1 with 52961 tokens (Total: 52961)\n",
            "\n",
            "Processing chunk 2/3\n",
            "Successfully created cache 2 with 40712 tokens (Total: 93673)\n",
            "\n",
            "Processing chunk 3/3\n",
            "Successfully created cache 3 with 42222 tokens (Total: 135895)\n",
            "\n",
            "Successfully processed 3 chunks (Total tokens: 135895)\n",
            "\n",
            "=== Analysis Results ===\n",
            "\n",
            "File: Sample Analysis\n",
            "Status: success\n",
            "Analysis:\n",
            "Let's analyze the emails for instances of \"ogs\" as you requested:\n",
            "\n",
            "**Emails containing \"ogs\":**\n",
            "\n",
            "* **EMAIL 13, 14:**  These emails contain \"logs\" in the context of \"project logs\" and  \"transport logs\".  This is a common business-related usage, and we can't deduce any fraudulent activity based on this.\n",
            "* **EMAIL 24, 25:** These emails reference \"Socal\" and \"San Juan\" which includes the word \"og\" but is related to specific locations.  This doesn't seem relevant to your request.\n",
            "* **EMAIL 52, 58:** These emails mention \"logs\" in a construction project context, such as \"gas line easement\" and \"detention/retention and filtration pond\".  This is again a common business-related term.\n",
            "\n",
            "**Emails containing other \"ogs\" words:**\n",
            "\n",
            "* **EMAIL 2:** \"golf\", \"bogs\" (if intended as a typo for \"dogs\") are mentioned, but are not related to the terms you specified.\n",
            "\n",
            "**Overall:**\n",
            "\n",
            "The emails primarily discuss business and financial transactions, real estate investments, and project management.  The instances of \"ogs\" are within normal, non-suspicious contexts.  There is no apparent evidence of fraudulent activities within these email snippets.  \n",
            "\n",
            "It's important to note that this analysis is based on a limited sample of emails. A more comprehensive review of all relevant communications, including those with external parties, would be necessary to draw definitive conclusions about potential fraud. \n",
            "\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#                    \"\\nPlease analyze these emails for signs of fraud, focusing on:\",\n",
        "#                    \"1. Financial irregularities\",\n",
        "#                    \"2. Suspicious transactions\",\n",
        "#                    \"3. Attempts to conceal information\",\n",
        "#                    \"4. Unusual communication patterns\",\n",
        "#                    \"\\nProvide a detailed analysis of any suspicious patterns or potential fraud indicators found.\""
      ],
      "metadata": {
        "id": "dCc_rXoWJuA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}