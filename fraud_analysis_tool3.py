# -*- coding: utf-8 -*-
"""fraud_analysis_tool2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gQJWrcNhDDHkbH0eDcfWkJnTxag67eoI

# Email Fraud Analysis
"""

!pip install gradio kagglehub google-generativeai pandas nltk

# Import required dependencies
import os
import re
import time
import logging
import pandas as pd
import numpy as np
import nltk
import kagglehub
import google.generativeai as genai
from google.generativeai import caching
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Download required NLTK data with proper error handling
try:
    nltk.download('punkt', quiet=True)
    logging.info("Successfully downloaded NLTK punkt resource")
except Exception as e:
    logging.error(f"Error downloading NLTK resources: {str(e)}")
    # Don't raise here - punkt might already be downloaded

# Configuration
@dataclass
class Config:
    """Simple configuration class for email analysis parameters"""
    # TESTING VALUE: Reduced to 1/10th for development
    # Original value was 1000, restore for production
    chunk_size: int = 100            # How many emails to process at once
    max_retries: int = 3             # Number of retries if API call fails
    min_message_length: int = 10     # Minimum length of message to process
    api_timeout: int = 30            # Timeout for API calls in seconds
    cache_ttl: int = 3600           # Cache lifetime in seconds (1 hour)
    model_name: str = 'gemini-1.5-flash-001'

# Create a single configuration instance with default values
config = Config()

# You can easily modify any settings here if needed
# For example:
# config.chunk_size = 500  # Process smaller chunks
# config.min_message_length = 20  # Require longer messages

# Set up Gemini API
os.environ['API_KEY'] = 'AIzaSyBPv2RhmsMT0HPcMyfibd2ELTuCAiCH1j0'  # Replace with your API key
genai.configure(api_key=os.environ['API_KEY'])

system_instruction = """
You are an expert fraud analyst.
Your task is to analyze the provided email chunks
and identify potential evidence of fraudulent activities.
Focus on:
1. Financial irregularities
2. Suspicious transactions
3. Attempts to conceal information
4. Unusual communication patterns
"""

def preprocess_message(message: str) -> str:
    """Clean and standardize message text"""
    if not isinstance(message, str):
        return ''

    message = message.strip()
    if len(message) < config.min_message_length:
        return ''

    return message

def chunk_message(message: str, max_chunk_size: int = 1000) -> List[str]:
    """Split message into smaller chunks while preserving context"""
    if not message:
        return []

    sentences = message.split('. ')  # Simple sentence splitting
    chunks = []
    current_chunk = []
    current_size = 0

    for sentence in sentences:
        sentence_size = len(sentence.split())
        if current_size + sentence_size > max_chunk_size and current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
            current_chunk = []
            current_size = 0
        current_chunk.append(sentence)
        current_size += sentence_size

    if current_chunk:
        chunks.append('. '.join(current_chunk) + '.')

    return chunks

# CSV Processing
def create_caches_from_csv(csv_path: str, chunk_size: int = None) -> List[caching.CachedContent]:
    """Process CSV data in chunks with proper validation and error handling"""
    chunk_size = chunk_size or config.chunk_size

    # Set a conservative token limit (30% of the max)
    MAX_TOKENS_PER_CACHE = 300000
    # Set total token limit across all caches (90% of API limit)
    TOTAL_TOKEN_LIMIT = 900000
    model = genai.GenerativeModel(config.model_name)

    try:
        df = pd.read_csv(csv_path)
        required_columns = ['file', 'message']
        if not all(col in df.columns for col in required_columns):
            missing = [col for col in required_columns if col not in df.columns]
            raise ValueError(f"Missing required columns: {missing}")

        caches = []
        num_chunks = len(df) // chunk_size + (len(df) % chunk_size > 0)

        # ============= TESTING LIMIT =============
        # REMOVE THIS LINE FOR PRODUCTION USE
        num_chunks = min(3, num_chunks)  # Limit to first 3 chunks for testing
        # ======================================

        # Count system instruction tokens once
        system_tokens = model.count_tokens(system_instruction).total_tokens
        print(f"System instruction uses {system_tokens} tokens")

        total_tokens_used = 0

        for chunk_num in range(num_chunks):
            # Check if we're approaching total token limit
            if total_tokens_used >= TOTAL_TOKEN_LIMIT:
                print(f"\nReached total token limit ({total_tokens_used} tokens) - stopping processing")
                break

            start_idx = chunk_num * chunk_size
            end_idx = min((chunk_num + 1) * chunk_size, len(df))

            chunk = df.iloc[start_idx:end_idx].copy()
            chunk.loc[:, 'message'] = chunk['message'].fillna('').astype(str)
            chunk.loc[:, 'message'] = chunk['message'].apply(preprocess_message)
            chunk = chunk[chunk['message'].str.len() > 0]

            if not chunk.empty:
                processed_messages = []
                current_token_count = system_tokens  # Start with system tokens
                print(f"\nProcessing chunk {chunk_num + 1}/{num_chunks}")

                for msg in chunk['message']:
                    msg_chunks = chunk_message(msg)
                    for msg_chunk in msg_chunks:
                        # Get exact token count for this chunk
                        chunk_tokens = model.count_tokens(msg_chunk).total_tokens

                        # Add safety margin for potential overhead (20%)
                        estimated_total = current_token_count + chunk_tokens + (chunk_tokens * 0.2)

                        if estimated_total > MAX_TOKENS_PER_CACHE:
                            print(f"Reached token limit at {current_token_count} tokens - creating cache...")
                            break

                        processed_messages.append(msg_chunk)
                        current_token_count += chunk_tokens

                    # Break out of outer loop too if we hit the limit
                    if estimated_total > MAX_TOKENS_PER_CACHE:
                        break

                if processed_messages:
                    try:
                        # Check if adding this cache would exceed total limit
                        if total_tokens_used + current_token_count > TOTAL_TOKEN_LIMIT:
                            print(f"\nWould exceed total token limit - stopping at {total_tokens_used} tokens")
                            break

                        cache = caching.CachedContent.create(
                            model=config.model_name,
                            display_name=f'fraud_analysis_chunk_{chunk_num}',
                            system_instruction=system_instruction,
                            contents=[{'text': msg} for msg in processed_messages],
                            ttl=datetime.timedelta(seconds=config.cache_ttl)
                        )
                        caches.append(cache)
                        total_tokens_used += current_token_count
                        print(f"Successfully created cache {chunk_num + 1} with {current_token_count} tokens (Total: {total_tokens_used})")
                    except Exception as e:
                        print(f"Warning: Failed to create cache for chunk {chunk_num + 1}: {str(e)}")
                        continue

        if not caches:
            raise ValueError("No valid caches could be created from the input data")

        print(f"\nSuccessfully processed {len(caches)} chunks (Total tokens: {total_tokens_used})")
        return caches

    except Exception as e:
        print(f"Error processing CSV: {str(e)}")
        raise

# Email Processing
def process_email_batch(emails: List[Dict], retries: Optional[int] = None) -> List[Dict]:
    """Process a batch of emails with retry logic"""
    retries = retries or config.max_retries
    results = []

    for email in emails:
        for attempt in range(retries):
            try:
                # Create a model instance for this analysis
                model = genai.GenerativeModel(config.model_name)

                # Analyze the email
                response = model.generate_content([
                    system_instruction,
                    f"From: {email.get('sender', 'Unknown')}\n"
                    f"To: {email.get('recipient', 'Unknown')}\n"
                    f"Subject: {email.get('subject', 'No Subject')}\n"
                    f"Body: {email.get('body', 'No Content')}"
                ])

                results.append({
                    'file': f"{email.get('sender', 'Unknown')} - {email.get('subject', 'No Subject')}",
                    'analysis': response.text,
                    'status': 'success'
                })
                break

            except Exception as e:
                if attempt == retries - 1:
                    logging.error(f"Failed to process email: {str(e)}")
                    results.append({
                        'file': f"{email.get('sender', 'Unknown')} - {email.get('subject', 'No Subject')}",
                        'error': str(e),
                        'status': 'failed'
                    })
                time.sleep(attempt * 2)  # Exponential backoff

    return results

"""## Interactive Interface Options

You can use either the Gradio web interface below for a graphical experience, or run the example code in the next cell for a programmatic approach.
"""

# Gradio Interface
import gradio as gr

def analyze_file(file, case_details, system_prompt=None, keywords=None):
    try:
        # If no file is uploaded, use the default dataset
        if file is None:
            path = kagglehub.dataset_download("wcukierski/enron-email-dataset")
            file_path = os.path.join(path, 'emails.csv')
            print(f"Using default email dataset from: {file_path}")
        else:
            file_path = file.name
            print(f"Using uploaded file: {file_path}")

        # Use custom system prompt if provided, otherwise use default
        current_prompt = system_prompt if system_prompt else system_instruction
        print(f"Using {'custom' if system_prompt else 'default'} system prompt")

        # Create caches from the CSV file
        caches = create_caches_from_csv(file_path)

        # Process a sample of emails using the cached content
        sample_results = []
        for cache in caches[:1]:  # Process first chunk for sample results
            try:
                # Create the model from cached content
                model = genai.GenerativeModel.from_cached_content(cache)

                # Read the DataFrame again to get the actual content for this chunk
                df = pd.read_csv(file_path)
                chunk_size = config.chunk_size
                chunk = df.iloc[0:chunk_size].copy()  # Get first chunk

                # Clean and prepare messages
                chunk.loc[:, 'message'] = chunk['message'].fillna('').astype(str)
                chunk.loc[:, 'message'] = chunk['message'].apply(preprocess_message)
                chunk = chunk[chunk['message'].str.len() > 0]

                # Format emails for analysis
                email_texts = [
                    f"EMAIL {i+1}:\n{msg}\n---"
                    for i, msg in enumerate(chunk['message']) if msg.strip()
                ]

                # Combine into analysis request
                analysis_prompt = [
                    current_prompt,
                    "Below are the email chunks to analyze for potential fraud:",
                    "\n".join(email_texts),
                    "\nPlease analyze these emails for signs of fraud, focusing on:",
                    "1. Financial irregularities",
                    "2. Suspicious transactions",
                    "3. Attempts to conceal information",
                    "4. Unusual communication patterns",
                    "\nProvide a detailed analysis of any suspicious patterns or potential fraud indicators found."
                ]

                # Generate the analysis
                response = model.generate_content(analysis_prompt)

                sample_results.append({
                    'file': 'Sample Analysis',
                    'analysis': response.text,
                    'status': 'success'
                })

            except Exception as e:
                print(f"Failed to process cached content: {str(e)}")
                sample_results.append({
                    'file': 'Error',
                    'error': str(e),
                    'status': 'failed'
                })

        # Convert results to DataFrame for display
        output_data = []
        for result in sample_results:
            row = {
                'File': result['file'],
                'Status': result['status'],
                'Analysis': result.get('analysis', result.get('error', ''))
            }
            output_data.append(row)

        results_df = pd.DataFrame(output_data)

        # Highlight keywords if provided
        if keywords:
            for keyword in keywords.split(','):
                keyword = keyword.strip()
                if keyword:
                    results_df['Analysis'] = results_df['Analysis'].str.replace(
                        f'({keyword})',
                        r'<mark>\1</mark>',
                        case=False,
                        regex=True
                    )
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        return results_df, timestamp
    except Exception as e:
        error_msg = f"Error: {str(e)}"
        print(error_msg)
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return pd.DataFrame([{
            'File': 'Error',
            'Status': 'failed',
            'Analysis': error_msg
        }]), timestamp

# Main execution
if __name__ == "__main__":
    # Create the Gradio interface
    with gr.Blocks() as demo:
        gr.Markdown("# Fraud Analysis Tool")
        gr.Markdown("*Upload a CSV file or leave empty to use the default dataset*")

        # Move Analyze button to top
        with gr.Row():
            analyze_btn = gr.Button("Analyze", variant="primary")

        with gr.Row():
            file_input = gr.File(label="Upload CSV File (Optional)")
            case_details = gr.Textbox(label="Case Details", placeholder="Enter case details...")

        with gr.Row():
            system_prompt_input = gr.Textbox(
                label="Custom System Prompt (Optional)",
                placeholder="Enter custom system prompt or leave empty to use default...",
                value=system_instruction,
                lines=5
            )

        with gr.Row():
            keyword_input = gr.Textbox(
                label="Highlight Keywords",
                placeholder="Enter keywords to highlight (comma-separated)..."
            )

        # Output area with download button and status
        with gr.Row():
            output_table = gr.Dataframe(
                headers=["File", "Status", "Analysis"],
                datatype=["str", "str", "markdown"],
                label="Analysis Results"
            )

        with gr.Row():
            download_btn = gr.Button("📥 Download Analysis Results", interactive=False)

        # Make status box larger to show processing logs
        with gr.Row():
            status_text = gr.Textbox(
                label="Processing Status",
                interactive=False,
                lines=10,  # Increased size to show more log content
                value="Ready to process..."
            )

        # Function to format the analysis for download
        def prepare_download(df):
            try:
                if df is None or df.empty:
                    return None, "No analysis results available. Please run analysis first."

                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"fraud_analysis_{timestamp}.csv"
                df.to_csv(filename, index=False)
                return filename, f"Analysis saved as {filename}"
            except Exception as e:
                return None, f"Error preparing download: {str(e)}"

        # Function to update download button state and maintain logs
        def update_download_state(df, current_status):
            new_status = current_status + "\nAnalysis complete. Ready for download."
            return gr.Button.update(interactive=True), df, new_status

        # Modified analyze_file wrapper to update status
        def analyze_with_status(file, case_details, system_prompt, keywords):
            status_updates = []

            def status_callback(msg):
                status_updates.append(msg)
                return gr.Textbox.update(value="\n".join(status_updates))

            try:
                # Start status
                status_callback("Starting analysis...")

                if file is None:
                    status_callback("Using default dataset...")
                else:
                    status_callback(f"Processing uploaded file: {file.name}")

                status_callback("Creating caches and processing chunks...")
                df = analyze_file(file, case_details, system_prompt, keywords)

                # Return both the results and the accumulated status
                return df, "\n".join(status_updates)
            except Exception as e:
                error_msg = f"Error during analysis: {str(e)}"
                status_callback(error_msg)
                return None, "\n".join(status_updates)

        # Set up the event handlers
        analyze_result = analyze_btn.click(
            analyze_with_status,
            inputs=[file_input, case_details, system_prompt_input, keyword_input],
            outputs=[output_table, status_text]
        ).then(
            update_download_state,
            inputs=[output_table, status_text],
            outputs=[download_btn, output_table, status_text]
        )

        download_btn.click(
            prepare_download,
            inputs=[output_table],
            outputs=[gr.File(label="Download Analysis"), status_text]
        )

    # Launch the interface
    demo.launch()